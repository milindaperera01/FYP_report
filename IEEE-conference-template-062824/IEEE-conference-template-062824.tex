\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{EEG-Based Stress Detection Using Deep Learning Techniques : A Survey\\
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Perera K.M.S.
\\Student}
\IEEEauthorblockA{\textit{Dept of Computer Engineering} \\
\textit{University of Peredeniya}\\
Sri Lanka\\
milindaperera14996@mail.com}
\and
\IEEEauthorblockN{ malisha
\\Student}
\IEEEauthorblockA{\textit{Dept of Computer Engineering} \\
\textit{University of Peredeniya}\\
Sri Lanka\\
milindaperera14996@mail.com}
\and
\IEEEauthorblockN{ nadeeka
\\Student}
\IEEEauthorblockA{\textit{Dept of Computer Engineering} \\
\textit{University of Peredeniya}\\
Sri Lanka\\
milindaperera14996@mail.com}
}

\maketitle

\begin{abstract}
Distinguishing between simple physiological arousal and the maladaptive state of true stress is crucial. 
This harmful condition arises when environmental demands exceed an organism's coping capacity, impairing physiological 
recovery. Due to its widespread negative impact, early and accurate detection of maladaptive stress is vital 
for protecting personal, social, and economic wellbeing in today’s fast-paced world. Electroencephalography (EEG), 
as a primary method for monitoring neural activity, has shown significant promise in identifying the distinct 
brain states associated with this stress. Modern deep learning architectures provide a powerful approach by 
automatically extracting meaningful patterns and hierarchical features from complex EEG data. This comprehensive 
survey offers researchers, practitioners, and technology enthusiasts a definitive overview of current advancements 
and highlights the future directions of EEG-based stress detection using deep learning.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}

\section{Introduction}
Stress, a biological response to internal or external stimuli, plays a critical role in exacerbating numerous pathological conditions\cite{yaribeygi2017impact}. 
It is generally classified as acute (short-term) or chronic (long-term), with unresolved chronic stress leading to serious health consequences 
by impairing the immune system and disrupting neuroendocrine functions. This persistent state is associated with a broad spectrum of illnesses, 
including cardiovascular disorders, diabetes, mental health conditions such as depression, and structural brain changes that negatively affect 
cognition and memory\cite{mariotti2015effects}. Recognizing the profound impact of stress, research has shifted toward identifying objective physiological markers 
beyond subjective assessments. Common approaches monitor autonomic nervous system responses through peripheral signals like Heart Rate Variability 
(HRV) from Electrocardiography (ECG), Galvanic Skin Response (GSR), and skin temperature\cite{kyrou2024deep}\cite{vanitha2013hybrid}\cite{herborn2015skin}. However, attention has increasingly turned to the brain’s 
electrical activity as the origin of the stress response, positioning Electroencephalogram (EEG) signals as a valuable, non-invasive, real-time tool 
for assessing neural dynamics. Traditional machine learning (ML) methods, especially Support Vector Machines (SVMs), have shown promise in 
classifying stress states from EEG data. Yet, these methods often depend heavily on manual, time-intensive feature extraction. In contrast, deep 
learning (DL) architectures offer a robust alternative by automatically learning meaningful features from complex, high-dimensional EEG signals. 
Models such as Convolutional Neural Networks (CNNs) excel at capturing spatial patterns, while Long Short-Term Memory (LSTM) networks effectively 
model temporal dependencies. Hybrid CNN-LSTM models are increasingly demonstrating superior classification accuracy by integrating both spatial and 
temporal information. This survey aims to deliver a thorough review of EEG-based stress detection using advanced deep learning techniques, analyzing 
current methodologies and performance trends, identifying key challenges, and exploring promising directions for future research.

\section{BACKGROUND}

\subsection{Stress}\label{AA}
Stress is a psychophysiological response of the human body to internal or external stressors, manifesting in physical, mental, or emotional forms. 
It is triggered when an individual perceives a situation as challenging, threatening, or demanding. 
Stressors may be biological, developmental, psychological, socio-cultural, or environmental, and even positive events can disrupt 
homeostasis, the body's internal balance. Stress is generally classified into two main categories: short-term (episodic) and long-term stress (chronic). 
Short-term stress arises from specific tasks or situations such as examinations, deadlines, or sudden pressures. Repeated exposure to such stressors 
may result in episodic stress, often linked to anxiety and hypertension \cite{kyrou2024deep}. Prolonged exposure leads to chronic stress, which can cause depression, 
psychological disorders, and severe physical illnesses. Chronic stress disrupts the nervous system, 
impairs functional capacity, and adversely affects daily life \cite{kyrou2024deep}.

Assessing stress is difficult because people react differently to the same stressor, and the same individual may react differently at different times. Clinical and 
psychological tools such as the Perceived Stress Scale are used for evaluation, but these survey-based methods are better suited for long-term psychological conditions and
 may not capture real-time stress fluctuations \cite{hafeez2024eeg}. In addition, physiological methods use bio-signals like ECG-based HRV, speech patterns, and galvanic skin response, all 
 of which change with mental stress. More recently, EEG has gained attention as a non-invasive and reliable signal for detecting stress-related brain activity, making it a key 
 focus in deep-learning-based stress detection research.

Today, stress has become a major public health concern, intensified by fast-paced lifestyles, heavy workloads, and increasing academic pressure on students. Prolonged or 
unmanaged stress is linked to serious consequences such as depression, cardiovascular disease, weakened immunity, violent behavior, and even suicidal tendencies. These 
risks highlight the growing need for early detection, continuous monitoring, and timely intervention. Establishing reliable and efficient stress-detection frameworks can 
greatly improve overall well-being by helping individuals manage stress more effectively. Such systems can contribute to better academic performance, enhanced workplace 
productivity, and more responsive medical care, making stress detection an essential component of modern health monitoring solutions.


\subsection{Deep Learning Architectures for EEG Analysis}
"----------------------"

\subsubsection{Introduction to Deep Learning in Neuroinformatics}
Deep learning (DL), a subfield of machine learning inspired by the hierarchical information-processing structure of the human brain, has emerged as a powerful paradigm for complex pattern recognition tasks. Unlike traditional machine learning methods, DL algorithms automatically learn multi-level feature representations, progressively transforming low-level inputs into more abstract and discriminative features \cite{li2020deep}. This capability has become especially valuable in the era of big data, where deep neural networks demonstrate exceptional flexibility in modeling complex, high-dimensional datasets and consistently achieving state-of-the-art performance across various domains \cite{li2020deep, merlin2022deep}.

In neuroinformatics, these strengths have led to the widespread adoption of DL techniques for Electroencephalogram (EEG) signal analysis \cite{gao2021complex, roy2023hybrid}. Their effectiveness is well-documented in applications such as automated epileptic seizure detection, where deep networks have shown a remarkable ability to decode intricate neural patterns \cite{tjepkema2018deep, birjandtalab2017automated}. While earlier studies on EEG-based stress detection predominantly employed traditional machine learning classifiers such as Support Vector Machines (SVM) and Naïve Bayes (NB) \cite{asif2019human}, these approaches often rely on handcrafted features and struggle to model the nonlinear and spatio-temporal complexity of EEG data. In contrast, deep learning’s capacity for end-to-end feature extraction and its ability to capture both spatial and temporal dependencies make it a compelling choice for developing more robust and accurate stress detection systems.

\subsubsection{Convolutional Neural Networks (CNNs)}
Convolutional Neural Networks (CNNs) are a class of deep learning models particularly adept at processing data with a grid-like topology, such as images or time-series signals \cite{james2020deep}. Their fundamental principle is \textit{automatic feature extraction}, which allows them to learn hierarchical representations directly from raw data, eliminating the need for manual feature engineering \cite{roy2023hybrid}.

Inspired by the hierarchical processing of the visual cortex, CNNs use layers of learnable filters that scan the input to detect local patterns, ranging from simple edges to complex, abstract structures \cite{james2020deep}. This ability to automatically discover relevant patterns from complex signals makes them exceptionally powerful for EEG analysis. For BCI applications such as stress detection, convolutional layers can implicitly learn the spatio-temporal features in EEG signals that correspond to different mental states, often outperforming traditional approaches \cite{roy2023hybrid}. The architecture of a CNN—particularly the number of layers, filter sizes, and pooling strategies—must be carefully tuned for optimal performance in a given EEG classification task \cite{james2020deep}.

\subsubsection{Recurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs)}
Recurrent Neural Networks (RNNs) are widely used for sequential data analysis, making them well-suited for modeling the temporal characteristics of EEG signals \cite{merlin2022deep, roy2023hybrid}. By maintaining internal hidden states, RNNs capture short-term temporal dependencies across successive time points. However, conventional RNNs often suffer from the vanishing gradient problem, limiting their ability to learn long-range temporal patterns in extended EEG recordings.

Long Short-Term Memory (LSTM) networks address this challenge through a gating mechanism that regulates the flow of information, enabling the model to retain or discard temporal features over long intervals \cite{roy2023hybrid}. LSTMs have therefore demonstrated high effectiveness in EEG applications such as seizure prediction, mental workload estimation, and cognitive state recognition \cite{merlin2022deep}. Their ability to learn complex, long-term temporal dependencies makes them a foundational architecture in many modern EEG-based systems.

\subsubsection{Gated Recurrent Units (GRU)}
The Gated Recurrent Unit (GRU), introduced by Cho et al. (2014), is a streamlined recurrent neural architecture designed to model long-term temporal dependencies in sequential data \cite{chung2014empirical}. GRUs mitigate common optimization issues in traditional RNNs—particularly vanishing and exploding gradients—using gating mechanisms that regulate information flow across time. Each GRU includes two gates: an \textit{update gate}, which determines how much of the past information is preserved, and a \textit{reset gate}, which controls how much of the previous state influences the current input processing.

This design enables GRUs to selectively retain useful temporal features while discarding less relevant information, enhancing gradient stability through an additive update process. Due to their efficiency and lower computational complexity relative to LSTMs, GRUs are widely used for modeling temporal patterns in EEG signals, particularly in applications requiring continuous and dynamic interpretation of brain activity \cite{chung2014empirical}.

\subsubsection{Curved Neural Networks}
Curved Neural Networks represent a recent advancement in geometric deep learning, extending conventional neural architectures to operate on curved, non-Euclidean manifolds rather than standard flat vector spaces \cite{aguilera2025explosive}. The core motivation for using curved spaces is that many real-world datasets—especially those containing hierarchical, relational, or tree-like structures—are more naturally embedded in geometries with constant curvature. For example, hyperbolic spaces, characterized by negative curvature and exponentially expanding distances, enable low-distortion embeddings of hierarchical structures and allow models to represent complex relationships more compactly and expressively \cite{aguilera2025explosive}.

Beyond hyperbolic geometry, the broader class of curved deep learning models incorporates higher-order interactions by deforming the statistical manifold on which learning occurs \cite{he2025hyperbolic}. Drawing from information geometry, these models employ deformed exponential families—such as Rényi-based formulations—to capture complex dependencies that are difficult to express in Euclidean models. Such characteristics make curved neural networks a theoretically grounded framework for EEG analysis, as EEG signals inherently exhibit nonlinear dynamics, hierarchical organization, and rich spatio-temporal relationships that benefit from non-Euclidean representations.


\subsection{An Introduction to Hyperbolic Geometry for Machine Learning}
The vast majority of deep learning models operate in Euclidean space, the familiar geometry of flat surfaces. However, certain data types, particularly those with an 
underlying hierarchical or tree-like structure, can be embedded into Euclidean space only with significant distortion (Ganea et al., 2018). Hyperbolic geometry, a type 
of non-Euclidean geometry characterized by constant negative curvature, provides a compelling alternative. In hyperbolic space, the volume grows 
exponentially with the radius, mirroring the exponential growth of nodes in a tree. This property allows hierarchical structures to be embedded 
with much lower distortion, preserving their metric properties more faithfully (He et al., 2025).
A common and convenient model for implementing hyperbolic geometry is the Poincaré ball model. This model represents an n-dimensional hyperbolic 
space as the interior of an n-dimensional unit ball, equipped with a specialized metric that causes distances to expand infinitely as they approach 
the boundary (Peng et al., 2022). To perform computations and build neural networks within this space, standard vector operations must be replaced 
with their hyperbolic equivalents, which are defined within the framework of Möbius gyrovector spaces. Key operations include:
\begin{itemize}
\item Möbius addition: A non-commutative and non-associative generalization of vector addition.
\item Möbius scalar multiplication: A generalization of scalar-vector multiplication.
\end{itemize}
These principled generalizations, along with corresponding definitions for operations like matrix-vector multiplication, allow for the development of
hyperbolic analogues of standard neural network layers (Ganea et al., 2018). By defining fully connected, recurrent, and other layers that operate 
entirely within the Poincaré ball, we can build deep learning models that are geometrically tailored to the intrinsic structure of hierarchical data, 
setting the stage for their application to complex EEG signals.
counting.

\section{A General Pipeline for EEG-Based Stress Detection}
Research in EEG-based stress detection typically follows a standardized, end-to-end process, beginning with data collection and 
culminating in model evaluation. This systematic pipeline ensures that experiments are reproducible and provides a common framework 
for comparing the efficacy of different methodologies. The following sections describe the key stages of this process, from acquiring 
and cleaning the raw signals to deriving inputs for classification models

\subsection{Data Acquisition and Public Datasets}\label{SCM}

The first step in any machine learning project is acquiring high-quality data. In the field of EEG-based emotion and stress recognition, a 
number of publicly available datasets have become benchmarks for evaluating new algorithms. These include:
\begin{itemize}
\item DEAP (Dataset for Emotion Analysis using Physiological signals)
\item SEED and SEED-IV (SJTU Emotion EEG Dataset)
\item DREAMER (Database for Emotion Recognition through EEG and ECG)
\end{itemize}
These datasets typically involve recording multi-channel EEG signals (e.g., 32 or 62 channels) from participants while they are exposed
to stimuli designed to elicit specific emotional states, such as watching carefully selected music video clips (Li et al., 2023)


\subsection{Signal Preprocessing and Artifact Removal}\label{AAA}
Raw EEG data is invariably contaminated by noise and artifacts from both physiological and environmental sources. Therefore, a critical 
preprocessing stage is required to clean the signals before any analysis can be performed. Key steps include:

Band-pass filtering: This is applied to isolate the frequency range of interest and remove unwanted noise. A common range used in emotion 
recognition studies is 4-45 Hz, which effectively removes low-frequency physiological artifacts (e.g., from breathing or heartbeats) and 
high-frequency environmental noise (e.g., 50/60 Hz power line interference) (Islam et al., 2021).

Artifact Removal: Specific techniques are used to remove artifacts caused by muscle movements (electromyography, EMG) and eye blinks 
(electrooculography, EOG). Independent Component Analysis (ICA) is a powerful method for separating these artifactual sources from the 
underlying brain signals (Katmah et al., 2021). Artifact Subspace

Reconstruction (ASR) is another technique that identifies and statistically interpolates high-variance signal components that exceed a 
certain threshold (Li et al., 2023).

Re-referencing: The recorded EEG potentials are relative. To standardize the signals and reduce common noise across channels, the data is 
often re-referenced. The Common Average Reference (CAR) method, which subtracts the average value of all electrodes from each individual 
electrode, is a widely recommended approach (Li et al., 2023)

\subsection{Feature Extraction Methodologies}\label{ITH}
Once the EEG signals are preprocessed, the next step is to derive inputs suitable for a machine learning model. This is governed by one 
of two primary philosophies for feature handling, which are increasingly combined in hybrid approaches.

The traditional philosophy involves manually calculating a set of features from the signal, often based on domain knowledge. These 
"hand-crafted" features are designed to capture specific characteristics of the EEG signal across different domains.

Frequency-Domain Features: These describe the power distribution across different frequency bands
\begin{itemize}
\item Power Spectral Density (PSD): The average power of the signal in a specific frequency band (e.g., Alpha, Beta) (Islam et al., 2021).
\item Differential Entropy (DE): A feature related to the complexity of the signal within a frequency band, often used in emotion 
recognition (Li et al., 2023)
\end{itemize}
Time-Frequency Domain Features: These capture how the frequency content of the signal changes over time
\begin{itemize}
\item Discrete Wavelet Transform (DWT): Decomposes the signal into different frequency sub-bands using a mother wavelet (e.g., 'db4'), 
providing both time and frequency information (Nirabi et al., 2021)
\item oShort-Time Fourier Transform (STFT): Calculates the frequency spectrum over short, overlapping time windows (Li et al., 2023).
\end{itemize}

Complexity and Non-linear Features: These measure the irregularity and unpredictability of the signal.

\begin{itemize}
\item Approximate Entropy (ApEn): A measure of the signal's regularity (Das Chakladar et al., 2020).
\item Shannon Entropy (SE): Quantifies the uncertainty or randomness in the signal (Islam et al., 2021).
\end{itemize}
The alternative, more modern philosophy is characteristic of many deep learning frameworks. In end-to-end learning, the raw or minimally
processed EEG signals are fed directly into a deep neural network (e.g., a CNN or RNN). The network itself is then tasked with learning
the most relevant and discriminative features automatically through its hierarchical layers (Li et al., 2023). This approach eliminates
the need for manual feature engineering, potentially allowing the model to discover novel patterns that are not captured by predefined
feature sets (Sundaresan et al., 2021). It is important to note that these two philosophies are not mutually exclusive; hybrid approaches
are common, where sophisticated deep learning architectures are trained on sets of highly engineered features to leverage both domain
knowledge and the model's capacity for learning complex relationships.

\section{Literature Review: Methodologies in Stress and Emotion Detection from EEG}\label{FAT}

This section surveys the evolution of classification techniques applied to EEG-based stress and emotion detection. The review begins 
by examining the performance of conventional "shallow" machine learning classifiers, which served as the baseline for many years. It 
then transitions to the now-dominant deep learning models that operate in standard Euclidean space, highlighting their architectural 
diversity and improved performance. Finally, this review culminates with an analysis of the emerging and theoretically compelling research 
on hyperbolic deep learning approaches, which are designed to better capture the intrinsic geometry of neural data

\subsection{Conventional Machine Learning Baselines}
Before the widespread adoption of deep learning, EEG-based classification tasks were primarily addressed using conventional machine learning 
algorithms. These "shallow" models rely on hand-crafted features extracted from the EEG signals. Common classifiers in this category include 
Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Linear Discriminant Analysis (LDA). Several studies have demonstrated the 
effectiveness of these methods. For instance, Nirabi et al. (2021) utilized features derived from the Discrete Wavelet Transform (DWT) and 
achieved a binary stress classification accuracy of 91.0\% with an SVM classifier. In a study involving a virtual reality environment, 
Kamińska et al. (2021) reported accuracies as high as 96.42\% for stress-versus-relaxation classification using both SVM and a Multilayer 
Perceptron (MLP), demonstrating the high performance achievable with well-engineered features and conventional classifiers.

\subsection{Euclidean Space Deep Learning Models}
Deep learning models operating in Euclidean space represent the current state-of-the-art, offering the ability to learn features automatically 
and often outperforming conventional methods.

CNN-Based Models: Convolutional Neural Networks are widely used to learn spatial patterns from EEG data. This is typically achieved by transforming 
the multi-channel EEG time-series into 2D images, for instance by arranging channels spatially or by using time-frequency representations like the 
Continuous Wavelet Transform (CWT) (Zeng et al., 2018). Fu et al. (2022) proposed a Symmetric Convolutional and Adversarial Neural Network (SDCAN) 
model that achieved an accuracy of 87.62\% for a challenging four-class stress classification task (before-stress, medium-stress, high-stress, 
and recovery).

RNN-Based Models: Recurrent Neural Networks, especially Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) variants, are adept at modeling 
the temporal dynamics inherent in EEG signals. Sundaresan et al. (2021) conducted a comparative study and found that a two-layer LSTM RNN was 
highly effective for multi-class stress classification, achieving an impressive average accuracy of 93.27\%. Another study implementing a 
GRU-based model for stress detection reported a test accuracy of 89\%, further validating the power of recurrent architectures for sequential 
EEG data ([Deep Learning Based, 2024]).

Hybrid Models: To capture both spatial and temporal dependencies simultaneously, many researchers have developed hybrid architectures that combine 
CNN and LSTM components. These models, often referred to as C-RNNs, first use convolutional layers to extract spatial features from EEG "frames" 
at each time step, and then feed the resulting sequence of feature vectors into an LSTM to model the temporal evolution of these features 
(Li et al., 2023).

\subsection{Hyperbolic Space Deep Learning Models}
A nascent but highly promising area of research involves moving beyond Euclidean geometry and applying deep learning models in hyperbolic space. The core motivation is both theoretical and empirical.
Theoretically, neuroscientists like Tozzi et al. (2018) have posited that the complex, non-linear dynamics of EEG signals can be effectively modeled on hyperbolic manifolds. This suggests that the 
intrinsic geometry of brain activity may be better represented in a negatively curved space.
Empirically, the first applications to EEG data have yielded strong results. Chang et al. (2025) developed a Multi-Scale Hyperbolic Contrastive 
Learning (MSHCL) method for cross-subject EEG emotion recognition. Their model achieved a notable 89.3\% accuracy on the three-class task of the 
public SEED dataset. Crucially, their ablation studies revealed that the performance gains from hyperbolic embeddings were most pronounced in 
datasets where hierarchical relationships in the data were more evident, providing direct support for the theoretical motivation.
While direct applications to stress detection are still emerging, the foundational tools for building such models already exist. 
Ganea et al. (2018) pioneered the development of hyperbolic versions of RNNs and LSTMs, providing the building blocks for modeling 
temporal data in hyperbolic space. More recently, Narayan and Patel (2024) defined hyperbolic classifier heads, including a hyperbolic 
cross-entropy loss function, which enables the final classification step to operate within the geometric constraints of the Poincaré ball. 
Together, these works demonstrate a clear path forward for developing end-to-end hyperbolic deep learning pipelines specifically designed 
to capitalize on the unique geometric properties of EEG data.

\section{Comparative Analysis and Discussion}

The literature review reveals a clear progression in the methodologies used for EEG-based stress and emotion detection, 
from feature-dependent shallow classifiers to powerful end-to-end deep learning frameworks. This section provides a direct 
comparison of these approaches, summarizing their architectural designs and reported performance. This synthesis culminates 
in a focused analysis of the unique advantages and differentiating factors that hyperbolic models bring to this domain.

\subsection{Performance and Architectural Comparison}
The following table summarizes representative studies from the literature, categorized by their underlying methodology. 
It highlights the diversity in model architectures, feature engineering techniques, and the performance levels achieved 
across different datasets and classification tasks.

\begin{table*}[ht]
\centering
\caption{Comparison of Conventional, Euclidean, and Hyperbolic Deep Learning Models for EEG-Based Stress Classification}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{2.5cm} p{3cm} p{2.5cm} p{4.2cm} p{2.5cm} p{3.2cm}}
\hline
\textbf{Reference(s)} & \textbf{Model / Architecture} & \textbf{Dataset(s) Used} & \textbf{Key Features / Preprocessing} & \textbf{Reported Accuracy / Performance} & \textbf{Key Contribution} \\
\hline

\multicolumn{6}{l}{\textbf{Conventional ML}} \\

Nirabi et al., 2021 &
SVM, k-NN, LDA &
Private &
Discrete Wavelet Transform (DWT), ‘db4’ wavelet &
SVM: 91.0\%; k-NN: 86.3\%; LDA: 90.0\% &
High performance with conventional SVM on DWT features. \\

\hline
\multicolumn{6}{l}{\textbf{Euclidean Deep Learning}} \\

Sundaresan et al., 2021 &
2-Layer LSTM RNN &
Private &
Raw EEG (end-to-end learning) &
93.27\% (4-class: Stressor, Baseline, Guided/Unguided Breathing) &
Demonstrated high accuracy of LSTM for multi-class stress classification. \\

Fu et al., 2022 &
SDCAN (Symmetric Convolutional \& Adversarial NN) &
Private (TSST-based) &
Raw EEG (end-to-end learning) &
87.62\% (4-class stress) &
Novel adversarial framework for subject-invariant EEG feature extraction. \\

Deep Learning Based, 2024 &
GRU with Adam optimizer &
Kaggle (DEAP subset) &
FFT applied to raw EEG &
89\% (Test accuracy for 3 classes) &
Effectiveness of GRU architecture for EEG stress classification. \\

Das Chakladar et al., 2020 &
Hybrid BLSTM-LSTM &
STEW Dataset &
PSD, statistical, non-linear (ApEn) features with GWO selection &
86.33\% (No task), 82.57\% (Multitasking) &
Used Grey Wolf Optimizer (GWO) for feature selection with a deep learning model. \\

\hline
\multicolumn{6}{l}{\textbf{Hyperbolic Deep Learning}} \\

Chang et al., 2025 &
MSHCL (Multi-Scale Hyperbolic Contrastive Learning) &
SEED, MPED, FACED &
End-to-end contrastive learning &
89.3\% (3-class on SEED) &
First hyperbolic contrastive learning for EEG, highlighting benefits for hierarchical structure. \\

Ganea et al., 2018 &
Hyperbolic RNN/GRU &
Textual Entailment, Noisy-Prefix Recognition &
Hyperbolic sentence embeddings &
97.14\% vs. 95.96\% (Euclidean GRU) &
Foundational work introducing hyperbolic RNN/GRU architectures. \\

\hline
\end{tabular}
\end{table*}


\subsection{Analysis of Hyperbolic Models' Differentiators}
The comparative results suggest that while well-tuned Euclidean deep learning models, particularly LSTMs, can achieve very high accuracy, the emerging field of hyperbolic deep learning offers unique and compelling advantages rooted in its geometric properties. The core differentiators of these models for EEG analysis are:
1.Superior Embedding of Hierarchies: The primary motivation for using hyperbolic geometry is its natural ability to represent hierarchical and tree-like data structures with very low distortion. Unlike Euclidean space, where the volume grows polynomially, the volume of hyperbolic space grows exponentially with the radius. This property mirrors the exponential growth of nodes in a tree, making it an ideal geometric substrate for embedding complex, branching data structures like those hypothesized to exist in brain connectivity patterns (Ganea et al., 2018; He et al., 2025). This property is theoretically ideal for modeling the complex, nested neural dynamics that contribute to inter-subject variability, a key challenge in creating generalizable EEG-based classifiers.
2.Increased Model Capacity: The exponential expansion of hyperbolic space provides a larger representational capacity. This means that complex data and decision boundaries can be modeled more efficiently within the space. For example, recent work has shown that associative memory networks built in negatively curved spaces demonstrate an increased memory capacity compared to their classical counterparts, allowing them to store and retrieve a greater number of patterns (Aguilera et al., 2025). This enhanced capacity could be highly beneficial for complex multi-class stress classification tasks.
3.Parameter Efficiency: By embedding data in a geometrically appropriate space, hyperbolic models have the potential to capture complex relationships more parsimoniously. They may be able to achieve comparable or better performance with fewer dimensions or fewer trainable parameters than equivalent Euclidean models, which might require much higher dimensions to "flatten out" the intrinsic curvature of the data (Liu et al., 2019). This efficiency could be crucial for deploying robust stress detection models on low-power, wearable EEG devices where computational resources are constrained.
Despite these clear theoretical and emerging empirical advantages, the application of hyperbolic deep learning to neurophysiological data is still in its infancy and is not without its own set of challenges and open questions that warrant further investigation.

\section{Challenges and Future Directions}
The application of deep learning to EEG-based stress detection has made significant strides, yet the field is still faced with persistent challenges. This section critically assesses the primary limitations that currently hinder progress and, based on these gaps, proposes promising avenues for future research. In particular, it highlights how the unique properties of hyperbolic models may offer novel solutions to some of these long-standing problems
\bibliographystyle{IEEEtran}
\bibliography{references}


\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
